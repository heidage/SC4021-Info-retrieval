{
    "train_loss": [
        0.02065748216179935,
        0.018920755994163062,
        0.01872054267196769,
        0.018829475107945894,
        0.018544652493131395,
        0.018114709607815666,
        0.017921628130256738,
        0.0182520577340926,
        0.017980995609449518,
        0.017745584197415922
    ],
    "train_metrics": {
        "accuracy": [
            0.6392785571142284,
            0.7334669338677354,
            0.7394789579158316,
            0.7374749498997996,
            0.7374749498997996,
            0.7374749498997996,
            0.7374749498997996,
            0.7394789579158316,
            0.7414829659318637,
            0.7414829659318637
        ],
        "f1": [
            0.28,
            0.06993006993006992,
            0.015151515151515152,
            0,
            0,
            0,
            0,
            0.015151515151515152,
            0.07194244604316548,
            0.0583941605839416
        ],
        "precision": [
            0.29411764705882354,
            0.4166666666666667,
            1.0,
            0,
            0,
            0,
            0,
            1.0,
            0.625,
            0.6666666666666666
        ],
        "recall": [
            0.26717557251908397,
            0.03816793893129771,
            0.007633587786259542,
            0.0,
            0.0,
            0.0,
            0.0,
            0.007633587786259542,
            0.03816793893129771,
            0.030534351145038167
        ]
    },
    "val_loss": [
        0.01652517913801906,
        0.017673287589180593,
        0.017294021640231624,
        0.016965230881598674,
        0.016926261499368895,
        0.01727432685179843,
        0.01692135764298857,
        0.016800138478477795,
        0.016880982199735526,
        0.017628406404077798
    ],
    "val_metrics": {
        "accuracy": [
            0.8021978021978022,
            0.8081918081918081,
            0.8101898101898102,
            0.8101898101898102,
            0.8101898101898102,
            0.8101898101898102,
            0.8101898101898102,
            0.8101898101898102,
            0.8041958041958042,
            0.7812187812187812
        ],
        "f1": [
            0.038834951456310676,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0.019999999999999997,
            0.026666666666666672
        ],
        "precision": [
            0.25,
            0.0,
            0,
            0,
            0,
            0,
            0,
            0,
            0.2,
            0.08571428571428572
        ],
        "recall": [
            0.021052631578947368,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.010526315789473684,
            0.015789473684210527
        ]
    },
    "test_loss": [],
    "test_metrics": {
        "accuracy": [],
        "f1": [],
        "precision": [],
        "recall": []
    },
    "steps": [
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "val_steps": [
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "grad_norms": [
        2.1704916994203813,
        2.5968792109342758,
        1.6853609360114206,
        3.4043816334742587,
        0.7805667280699708,
        0.5463461524850572,
        1.6398060551728122,
        0.6620668783871224,
        1.023511060178862,
        2.249396827304736,
        2.12847509860876,
        1.6657614322321024,
        1.004669753179769,
        0.745998814527411,
        0.3406762508539032,
        2.481418240815401,
        0.9131857384491013,
        0.5195878866070416,
        0.6047150790764135,
        0.2979874134362035,
        0.5517032474381267,
        0.5829170860815793,
        0.35135482443729416,
        0.3364604050802882,
        0.5041978760855272,
        0.700402147253044,
        1.1127409575638012,
        0.9396547626165557,
        1.7288902190339286,
        0.665666550048627,
        0.4653507106595498,
        1.210375133116031,
        0.26150981138016505,
        1.483276726125041,
        0.5599351163746178,
        0.3116746883606538,
        0.5029486817875295,
        0.7767077292310205,
        0.4369704623022699,
        0.46689373566186987,
        0.5866494339570636,
        0.7021259672183078,
        0.9465428610092204,
        1.4364209530176595,
        0.49068139455630444,
        0.6252255694416817,
        1.2152539725793758,
        1.109796099117375,
        0.9501849756634329,
        0.35625412488298025,
        1.4986067849677056,
        1.0126018019509502,
        1.1235367823392153,
        0.5778366514423396,
        0.4256039919855539,
        0.6567335338913836,
        0.6930974750721361,
        0.6955253044579877,
        0.6432071471353993,
        0.7538414953305619,
        1.277034990635002,
        0.9687986863427795,
        0.41000105350394733,
        0.9964511773432605,
        0.8356058842327911,
        0.6721367747231852,
        1.5947258974192664,
        2.6719405499170534,
        0.6874203720362857,
        0.35378452099394053,
        0.5398981540056411,
        1.1083862344385125,
        1.0262219464384543,
        0.5352626492604031,
        0.4804410659271525,
        2.3430439211660996,
        1.8325166948488913,
        0.4865813146097935,
        0.8453122465871274,
        0.5438703566805998,
        0.5732631867576856,
        1.0465488979243673,
        0.7263998216367327,
        1.1188923500594683,
        1.088420196203515,
        0.37564894763636403,
        0.42861606445512734,
        0.742554497148376,
        0.6034005190012977,
        0.7569735592696816,
        0.5914391168334987,
        0.5834544277167879,
        0.9416069377330132,
        0.7250697064737324,
        0.47703355972771533,
        0.5620184949366376,
        1.462385315739084,
        1.3888837264385074,
        1.4393084450857714,
        0.8523349316019448,
        1.0826131812063977,
        1.2149686281918548,
        0.7358868306328077,
        1.6821875305613503,
        1.30847874727624,
        1.1154680774197914,
        0.4943778832384851,
        0.3543494036712218,
        0.797900308854878,
        1.3787969707045704,
        0.5063326369490824,
        1.0121028154389933,
        0.4212300985818729,
        0.6042841190937907,
        1.481960304779932,
        0.4771457342139911,
        0.8671178121585399,
        0.5500641281259959,
        1.999546081176959,
        0.5089371479698457,
        0.33128126022438664,
        1.4049093497451395,
        0.5575288644758984,
        0.3018224341212772,
        1.057160449679941,
        2.1199283886235207,
        0.42069057433400303,
        1.6699286954244599,
        1.9142944453051314,
        0.42017875993042253,
        0.6264447523280978,
        1.2124712289078161,
        1.0083002886385657,
        1.4933642643736675,
        0.979700944502838,
        2.0638288233894855,
        0.4907920251134783,
        1.1625034278258681,
        0.4487450942688156,
        0.3468963361956412,
        0.588955026374606,
        0.4221839378005825,
        1.3162830749060959,
        1.4195675509399734,
        0.40003955687279813,
        0.5791167054485413,
        1.0585895025869831,
        0.8545873945695348,
        0.8810495326761156,
        1.3337783657480031,
        0.8004674686817452,
        0.7913875333615579,
        0.5274796964076813,
        0.6798225787933916,
        1.5682445939164609,
        0.7436689578462392,
        1.1809678308200091,
        1.3510937220416963,
        0.7406297814450227,
        0.8369510857737623
    ],
    "grad_steps": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16,
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24,
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32,
        33,
        34,
        35,
        36,
        37,
        38,
        39,
        40,
        41,
        42,
        43,
        44,
        45,
        46,
        47,
        48,
        49,
        50,
        51,
        52,
        53,
        54,
        55,
        56,
        57,
        58,
        59,
        60,
        61,
        62,
        63,
        64,
        65,
        66,
        67,
        68,
        69,
        70,
        71,
        72,
        73,
        74,
        75,
        76,
        77,
        78,
        79,
        80,
        81,
        82,
        83,
        84,
        85,
        86,
        87,
        88,
        89,
        90,
        91,
        92,
        93,
        94,
        95,
        96,
        97,
        98,
        99,
        100,
        101,
        102,
        103,
        104,
        105,
        106,
        107,
        108,
        109,
        110,
        111,
        112,
        113,
        114,
        115,
        116,
        117,
        118,
        119,
        120,
        121,
        122,
        123,
        124,
        125,
        126,
        127,
        128,
        129,
        130,
        131,
        132,
        133,
        134,
        135,
        136,
        137,
        138,
        139,
        140,
        141,
        142,
        143,
        144,
        145,
        146,
        147,
        148,
        149,
        150,
        151,
        152,
        153,
        154,
        155,
        156,
        157,
        158,
        159,
        160
    ]
}