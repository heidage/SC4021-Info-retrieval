{
    "train_loss": [
        0.019860171749697703,
        0.01920097899365876,
        0.018747821906704064,
        0.01855932806594003,
        0.018353985141546122,
        0.01866322406261277,
        0.01809335367349712,
        0.018060005591618584,
        0.017763403875410165,
        0.017273020290303975
    ],
    "train_metrics": {
        "accuracy": [
            0.7334669338677354,
            0.7374749498997996,
            0.7374749498997996,
            0.7374749498997996,
            0.7374749498997996,
            0.7374749498997996,
            0.7414829659318637,
            0.7374749498997996,
            0.7414829659318637,
            0.7454909819639278
        ],
        "f1": [
            0.014814814814814814,
            0,
            0,
            0,
            0,
            0,
            0.044444444444444446,
            0.015037593984962405,
            0.07194244604316548,
            0.072992700729927
        ],
        "precision": [
            0.25,
            0,
            0,
            0,
            0,
            0,
            0.75,
            0.5,
            0.625,
            0.8333333333333334
        ],
        "recall": [
            0.007633587786259542,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.022900763358778626,
            0.007633587786259542,
            0.03816793893129771,
            0.03816793893129771
        ]
    },
    "val_loss": [
        0.00924470039414397,
        0.01064146732743312,
        0.010383926183244007,
        0.009996592969400808,
        0.010262297346748205,
        0.01008291395394028,
        0.010458560867442025,
        0.009767694219287174,
        0.009930923092825752,
        0.00922503577829856
    ],
    "val_metrics": {
        "accuracy": [
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            1.0,
            0.996003996003996,
            1.0,
            0.99000999000999,
            0.991008991008991
        ],
        "f1": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
        ],
        "precision": [
            0,
            0,
            0,
            0,
            0,
            0,
            0.0,
            0,
            0.0,
            0.0
        ],
        "recall": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
        ]
    },
    "test_loss": [],
    "test_metrics": {
        "accuracy": [],
        "f1": [],
        "precision": [],
        "recall": []
    },
    "steps": [
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "val_steps": [
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
    ],
    "grad_norms": [
        2.5738054994726554,
        2.392459204129409,
        3.0463262007397134,
        0.9379789271843038,
        2.7793281790509354,
        0.45499666060641175,
        0.39398322943270614,
        1.1123209232609952,
        0.9914693950704532,
        0.425797264180801,
        0.7116650383586602,
        0.4624486489992705,
        0.5404666755275684,
        0.7111692465696251,
        0.9323647296696436,
        0.7247953011246864,
        0.5986759921215707,
        0.9143540213990491,
        0.9529886263771914,
        0.6510340194945456,
        1.72266043954005,
        1.4764007653138833,
        1.8366637204162544,
        0.6824440030541155,
        0.644678129508975,
        0.8380330522340955,
        1.9109306989703327,
        0.6355207168676316,
        0.6864660749342875,
        0.37578194780508056,
        1.4357976819155738,
        0.5857007594895549,
        0.9038913235999644,
        0.4842915950794122,
        1.3818924767401768,
        0.3860200370604616,
        0.6957693452714011,
        0.6209508838583133,
        0.9403245440043975,
        1.636387592589017,
        1.033842908742372,
        0.970287369156722,
        0.6317353603662923,
        1.1378744024405023,
        1.1416743645386305,
        1.0661381677200552,
        0.5637332335318206,
        1.0025835449487204,
        0.6325805413798662,
        0.693221838446334,
        0.5951322887049173,
        0.5801734274573391,
        1.1047813253244385,
        0.6116023016074905,
        0.9049697407608619,
        0.48089250798511785,
        0.2519964280363638,
        0.479090197768528,
        0.5847205599566223,
        0.9177141001346172,
        0.975545108289225,
        2.3877369403780904,
        0.6128288108011475,
        0.890075105617143,
        0.3971077964815777,
        1.2506102089973865,
        0.42785071417165454,
        0.9881209846353158,
        0.809096526238136,
        1.5821074485138524,
        1.6803043825202622,
        0.781446290522581,
        0.41813373037439305,
        0.6998554638994392,
        0.4652971496398095,
        0.6912173684067966,
        0.4483813302467752,
        0.6275589062424842,
        0.7207175696094055,
        0.8464604769542348,
        0.5862736058115843,
        1.089480783091858,
        0.3990467523326515,
        0.3925367476622341,
        0.6570063088438474,
        0.8380106048862217,
        0.6809817524626851,
        0.6326131602691021,
        0.6122832050459692,
        0.44647936208639294,
        0.4979798593412852,
        0.540917884951341,
        0.5334184472849302,
        1.7068212521262467,
        1.2031631938298233,
        3.00671251071617,
        0.9107615174725652,
        1.6538644102402031,
        0.8159357390250079,
        1.0589241147972643,
        0.32531592206214555,
        1.1259697065106593,
        0.6930906465131557,
        0.4918820120219607,
        0.7698339288472198,
        1.3258434879244305,
        0.35358229058692814,
        0.8016384931615903,
        0.5090347587247379,
        0.7035624771961011,
        0.5872811172739603,
        0.6237691538699437,
        0.34544800781441154,
        0.8122094236059638,
        1.1223199433879927,
        1.385637712024618,
        0.3812241688137874,
        1.446775919990614,
        0.3107708287643618,
        0.6147947692843445,
        1.0574649134796346,
        1.6162260877899826,
        0.530685841345985,
        0.32900283300841693,
        0.5380648264290357,
        1.3003846065257676,
        1.285346039221622,
        1.966490786173381,
        1.2810179472544405,
        0.5386318512901198,
        1.4065342420944944,
        0.44106885968358256,
        0.5067923912138212,
        1.185718854307197,
        0.8638799290274619,
        0.47502706228988245,
        2.4463030683109537,
        1.8656219380209222,
        1.2240334160160273,
        1.081042424077168,
        1.1077610173961148,
        1.67651008348912,
        0.655839899147395,
        1.3831786240334623,
        0.7956140955211595,
        0.9184725929226261,
        2.4271223433315754,
        0.40891192128765397,
        2.0047039133496583,
        0.823820389341563,
        1.2094990034820512,
        0.43914847725682193,
        0.44654856382112484,
        0.6078775682253763,
        1.2260517036847887,
        2.1574821462854743,
        1.6043991592014208,
        0.9170443237526342,
        0.43525544385192916,
        1.2790513430954888
    ],
    "grad_steps": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16,
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24,
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32,
        33,
        34,
        35,
        36,
        37,
        38,
        39,
        40,
        41,
        42,
        43,
        44,
        45,
        46,
        47,
        48,
        49,
        50,
        51,
        52,
        53,
        54,
        55,
        56,
        57,
        58,
        59,
        60,
        61,
        62,
        63,
        64,
        65,
        66,
        67,
        68,
        69,
        70,
        71,
        72,
        73,
        74,
        75,
        76,
        77,
        78,
        79,
        80,
        81,
        82,
        83,
        84,
        85,
        86,
        87,
        88,
        89,
        90,
        91,
        92,
        93,
        94,
        95,
        96,
        97,
        98,
        99,
        100,
        101,
        102,
        103,
        104,
        105,
        106,
        107,
        108,
        109,
        110,
        111,
        112,
        113,
        114,
        115,
        116,
        117,
        118,
        119,
        120,
        121,
        122,
        123,
        124,
        125,
        126,
        127,
        128,
        129,
        130,
        131,
        132,
        133,
        134,
        135,
        136,
        137,
        138,
        139,
        140,
        141,
        142,
        143,
        144,
        145,
        146,
        147,
        148,
        149,
        150,
        151,
        152,
        153,
        154,
        155,
        156,
        157,
        158,
        159,
        160
    ]
}