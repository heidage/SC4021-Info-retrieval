{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f8da88-ef68-4226-a519-e2df983c344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24517bc3-4f2c-4131-b5cc-2e55632da6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4353fa0c-5283-4f87-a2d0-26491fd30363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f4120c-3c50-411d-83dc-8d3a3a0ae2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7377dbfc-1e20-414a-b3ff-f6f82cde0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# Function to clean text: Tokenization, Lemmatization, and Stopword removal\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the text is a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        tokens = word_tokenize(text)  # Tokenize the text\n",
    "\n",
    "        # Lemmatize each token\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        # Additional preprocessing (like removing stopwords)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens_without_stopwords = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "\n",
    "        # Rejoin tokens into a single string before passing to TextBlob\n",
    "        cleaned_text = \" \".join(tokens_without_stopwords)\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        # If the text is not a string (e.g., NaN or float), return an empty string or a default value\n",
    "        return \"\"  # Or return a string like 'Invalid Text' if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7416ae-0f0d-4b45-bffb-f7f3c77f30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    # Step 1: Use spaCy to extract entities (like companies, products, etc.)\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]  # Extracting organizations\n",
    "\n",
    "    # Step 2: Apply POS tagging to the tokens in the text\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    pos_tags = pos_tag(tokens)  # Get POS tags for the tokens\n",
    "\n",
    "    # Step 3: Extract proper nouns (NNP) or important nouns (NN) using POS tagging\n",
    "    nouns = [word for word, tag in pos_tags if tag in [\"NNP\",\"NN\"]]  # Extracting proper nouns and common nouns\n",
    "\n",
    "    # Combine NER and POS results\n",
    "    refined_entities = list(set(entities + nouns))  # Combine and remove duplicates\n",
    "    return refined_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a55edd0-42f8-49c0-a87c-06da48902375",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Function to load data from Excel\n",
    "def load_data_from_excel(file_path):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Assuming the Excel file has columns 'text' for the comments\n",
    "    texts = df['cleaned_body'].tolist()  # List of texts (Reddit posts)\n",
    "    return df, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91bb5860-3a37-411a-a439-5e81e8417df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sentiment analysis with TextBlob\n",
    "def analyze_sentiment_with_textblob(texts):\n",
    "    subjectivity = []\n",
    "    polarity = []\n",
    "    for text in texts:\n",
    "        blob = TextBlob(text)\n",
    "        subjectivity.append(blob.sentiment.subjectivity)\n",
    "        polarity.append(blob.sentiment.polarity)\n",
    "    return subjectivity, polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd81035e-9688-4f7f-b838-0cfd54489cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Time: 1.7118 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load data from Excel sheet\n",
    "file_path = \"/Users/jaredog/Downloads/cleaned_stock_data (1).csv\"  # Replace with your actual Excel file path\n",
    "df, texts = load_data_from_excel(file_path)\n",
    "\n",
    "# --- 1. Preprocess the text ---\n",
    "start_time = time.time()\n",
    "cleaned_texts = [clean_text(text) for text in texts]\n",
    "preprocessing_time = time.time() - start_time\n",
    "print(f\"Preprocessing Time: {preprocessing_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37830680-052e-41ad-a8db-77285202f7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Time: 0.8853 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Sentiment Analysis with TextBlob ---\n",
    "start_time = time.time()\n",
    "subjectivity, polarity = analyze_sentiment_with_textblob(cleaned_texts)\n",
    "sentiment_analysis_time = time.time() - start_time\n",
    "print(f\"Sentiment Analysis Time: {sentiment_analysis_time:.4f} seconds\")\n",
    "\n",
    "# Append the sentiment analysis results to the dataframe\n",
    "df['subjectivity'] = subjectivity\n",
    "df['polarity'] = polarity\n",
    "\n",
    "# Determine the overall sentiment (positive, negative, neutral)\n",
    "sentiment_labels = []\n",
    "for polarity_score in polarity:\n",
    "    if polarity_score > 0:\n",
    "        sentiment_labels.append('positive')\n",
    "    elif polarity_score < 0:\n",
    "        sentiment_labels.append('negative')\n",
    "    else:\n",
    "        sentiment_labels.append('neutral')\n",
    "\n",
    "df['sentiment'] = sentiment_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0d99a3d-e310-43df-98a9-857afb21e144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>datetime</th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned_body</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>2025-03-01 10:00:42</td>\n",
       "      <td>1j0w73o</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>79</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Please use this thread to discuss your portfol...</td>\n",
       "      <td>Please use this thread to discuss your portfol...</td>\n",
       "      <td>0.36803</td>\n",
       "      <td>0.176667</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-04 13:54:48</td>\n",
       "      <td>mfykqf0</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>Jimlad73</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100% S&amp;P and bricking it</td>\n",
       "      <td>100% S&amp;P and bricking it</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-11 00:18:18</td>\n",
       "      <td>mh4b63d</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>inopia</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100% 6-month tbills, lol :)</td>\n",
       "      <td>100% 6-month tbills, lol :)</td>\n",
       "      <td>0.85000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-11 21:38:43</td>\n",
       "      <td>mha0zp1</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>thenuttyhazlenut</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>|Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...</td>\n",
       "      <td>|Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...</td>\n",
       "      <td>0.49250</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-13 15:57:56</td>\n",
       "      <td>mhl2a2j</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>elgrandorado</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ticker &amp; %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...</td>\n",
       "      <td>Ticker &amp; %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type             datetime  post_id subreddit  \\\n",
       "0     post  2025-03-01 10:00:42  1j0w73o    stocks   \n",
       "1  comment  2025-03-04 13:54:48  mfykqf0    stocks   \n",
       "2  comment  2025-03-11 00:18:18  mh4b63d    stocks   \n",
       "3  comment  2025-03-11 21:38:43  mha0zp1    stocks   \n",
       "4  comment  2025-03-13 15:57:56  mhl2a2j    stocks   \n",
       "\n",
       "                                               title            author  \\\n",
       "0  Rate My Portfolio - r/Stocks Quarterly Thread ...     AutoModerator   \n",
       "1  Rate My Portfolio - r/Stocks Quarterly Thread ...          Jimlad73   \n",
       "2  Rate My Portfolio - r/Stocks Quarterly Thread ...            inopia   \n",
       "3  Rate My Portfolio - r/Stocks Quarterly Thread ...  thenuttyhazlenut   \n",
       "4  Rate My Portfolio - r/Stocks Quarterly Thread ...      elgrandorado   \n",
       "\n",
       "                                                 url  upvotes  downvotes  \\\n",
       "0  https://www.reddit.com/r/stocks/comments/1j0w7...       79        4.0   \n",
       "1  https://www.reddit.com/r/stocks/comments/1j0w7...       12        NaN   \n",
       "2  https://www.reddit.com/r/stocks/comments/1j0w7...       11        NaN   \n",
       "3  https://www.reddit.com/r/stocks/comments/1j0w7...        6        NaN   \n",
       "4  https://www.reddit.com/r/stocks/comments/1j0w7...        7        NaN   \n",
       "\n",
       "   upvote_ratio                                               body  \\\n",
       "0          0.95  Please use this thread to discuss your portfol...   \n",
       "1           NaN                           100% S&P and bricking it   \n",
       "2           NaN                        100% 6-month tbills, lol :)   \n",
       "3           NaN  |Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...   \n",
       "4           NaN  Ticker & %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...   \n",
       "\n",
       "                                        cleaned_body  subjectivity  polarity  \\\n",
       "0  Please use this thread to discuss your portfol...       0.36803  0.176667   \n",
       "1                           100% S&P and bricking it       0.00000  0.000000   \n",
       "2                        100% 6-month tbills, lol :)       0.85000  0.650000   \n",
       "3  |Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...       0.49250  0.267500   \n",
       "4  Ticker & %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...       0.00000  0.000000   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1   neutral  \n",
       "2  positive  \n",
       "3  positive  \n",
       "4   neutral  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 3. Remove neutral sentiment rows ---\n",
    "#df = df[df['sentiment'] != 'neutral']  # Drop all rows where sentiment is 'neutral'\n",
    "\n",
    "df['cleaned_body'].tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a27231a-4982-4124-b5a7-1860ed6d60ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Extraction Time: 69.4438 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>datetime</th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned_body</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>2025-03-01 10:00:42</td>\n",
       "      <td>1j0w73o</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>79</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Please use this thread to discuss your portfol...</td>\n",
       "      <td>Please use this thread to discuss your portfol...</td>\n",
       "      <td>0.36803</td>\n",
       "      <td>0.176667</td>\n",
       "      <td>positive</td>\n",
       "      <td>[Losing, paper, portfolio, %, /, book, learn, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-04 13:54:48</td>\n",
       "      <td>mfykqf0</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>Jimlad73</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100% S&amp;P and bricking it</td>\n",
       "      <td>100% S&amp;P and bricking it</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[S, P, %, S&amp;P]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-11 00:18:18</td>\n",
       "      <td>mh4b63d</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>inopia</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100% 6-month tbills, lol :)</td>\n",
       "      <td>100% 6-month tbills, lol :)</td>\n",
       "      <td>0.85000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>positive</td>\n",
       "      <td>[%, lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-11 21:38:43</td>\n",
       "      <td>mha0zp1</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>thenuttyhazlenut</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>|Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...</td>\n",
       "      <td>|Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...</td>\n",
       "      <td>0.49250</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>positive</td>\n",
       "      <td>[Design|12.75, |PBR|Petroleo, |WISE, insurance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-13 15:57:56</td>\n",
       "      <td>mhl2a2j</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>elgrandorado</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ticker &amp; %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...</td>\n",
       "      <td>Ticker &amp; %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[ASML, MA, NTDOY, SPGI, V, GOOG, MANH, Ticker,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type             datetime  post_id subreddit  \\\n",
       "0     post  2025-03-01 10:00:42  1j0w73o    stocks   \n",
       "1  comment  2025-03-04 13:54:48  mfykqf0    stocks   \n",
       "2  comment  2025-03-11 00:18:18  mh4b63d    stocks   \n",
       "3  comment  2025-03-11 21:38:43  mha0zp1    stocks   \n",
       "4  comment  2025-03-13 15:57:56  mhl2a2j    stocks   \n",
       "\n",
       "                                               title            author  \\\n",
       "0  Rate My Portfolio - r/Stocks Quarterly Thread ...     AutoModerator   \n",
       "1  Rate My Portfolio - r/Stocks Quarterly Thread ...          Jimlad73   \n",
       "2  Rate My Portfolio - r/Stocks Quarterly Thread ...            inopia   \n",
       "3  Rate My Portfolio - r/Stocks Quarterly Thread ...  thenuttyhazlenut   \n",
       "4  Rate My Portfolio - r/Stocks Quarterly Thread ...      elgrandorado   \n",
       "\n",
       "                                                 url  upvotes  downvotes  \\\n",
       "0  https://www.reddit.com/r/stocks/comments/1j0w7...       79        4.0   \n",
       "1  https://www.reddit.com/r/stocks/comments/1j0w7...       12        NaN   \n",
       "2  https://www.reddit.com/r/stocks/comments/1j0w7...       11        NaN   \n",
       "3  https://www.reddit.com/r/stocks/comments/1j0w7...        6        NaN   \n",
       "4  https://www.reddit.com/r/stocks/comments/1j0w7...        7        NaN   \n",
       "\n",
       "   upvote_ratio                                               body  \\\n",
       "0          0.95  Please use this thread to discuss your portfol...   \n",
       "1           NaN                           100% S&P and bricking it   \n",
       "2           NaN                        100% 6-month tbills, lol :)   \n",
       "3           NaN  |Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...   \n",
       "4           NaN  Ticker & %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...   \n",
       "\n",
       "                                        cleaned_body  subjectivity  polarity  \\\n",
       "0  Please use this thread to discuss your portfol...       0.36803  0.176667   \n",
       "1                           100% S&P and bricking it       0.00000  0.000000   \n",
       "2                        100% 6-month tbills, lol :)       0.85000  0.650000   \n",
       "3  |Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...       0.49250  0.267500   \n",
       "4  Ticker & %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...       0.00000  0.000000   \n",
       "\n",
       "  sentiment                                           entities  \n",
       "0  positive  [Losing, paper, portfolio, %, /, book, learn, ...  \n",
       "1   neutral                                     [S, P, %, S&P]  \n",
       "2  positive                                           [%, lol]  \n",
       "3  positive  [Design|12.75, |PBR|Petroleo, |WISE, insurance...  \n",
       "4   neutral  [ASML, MA, NTDOY, SPGI, V, GOOG, MANH, Ticker,...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 4. Named Entity Recognition ---\n",
    "start_time = time.time()\n",
    "# Apply NER only to the remaining (non-neutral) rows\n",
    "# make sure no NaNs, and everything is a str\n",
    "df['cleaned_body'] = df['cleaned_body'].fillna(\"\").astype(str) #added\n",
    "remaining_texts = df['cleaned_body'].tolist()  # Only the rows with non-neutral sentiment\n",
    "entities = [extract_entities(text) for text in remaining_texts]\n",
    "entity_extraction_time = time.time() - start_time\n",
    "print(f\"Entity Extraction Time: {entity_extraction_time:.4f} seconds\")\n",
    "\n",
    "# Append extracted entities to the dataframe\n",
    "df['entities'] = entities\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2225c763-97ff-4fcb-96bb-c4c848023266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified data saved to Sentimented(Neutral).csv\n",
      "\n",
      "--- Performance Metrics ---\n",
      "Total Time for Preprocessing, Sentiment Analysis, and Entity Extraction: 72.0409 seconds\n",
      "Records Classified per Second: 140.57 records/second\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Save the modified data back to CSV ---\n",
    "output_file_path = \"Sentimented(Neutral).csv\"\n",
    "df.to_csv(output_file_path, index=False)\n",
    "print(f\"Modified data saved to {output_file_path}\")\n",
    "\n",
    "# --- Performance Metrics ---\n",
    "total_time = preprocessing_time + sentiment_analysis_time + entity_extraction_time\n",
    "records_per_second = len(df) / total_time  # records classified per second\n",
    "\n",
    "print(f\"\\n--- Performance Metrics ---\")\n",
    "print(f\"Total Time for Preprocessing, Sentiment Analysis, and Entity Extraction: {total_time:.4f} seconds\")\n",
    "print(f\"Records Classified per Second: {records_per_second:.2f} records/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85b0c011-064c-4304-8004-349032bec376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>datetime</th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned_body</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>2025-03-01 10:00:42</td>\n",
       "      <td>1j0w73o</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>79</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Please use this thread to discuss your portfol...</td>\n",
       "      <td>Please use this thread to discuss your portfol...</td>\n",
       "      <td>0.36803</td>\n",
       "      <td>0.176667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Losing', 'paper', 'portfolio', '%', '/', 'bo...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-04 13:54:48</td>\n",
       "      <td>mfykqf0</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>Jimlad73</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100% S&amp;P and bricking it</td>\n",
       "      <td>100% S&amp;P and bricking it</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['S', 'P', '%', 'S&amp;P']</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-11 00:18:18</td>\n",
       "      <td>mh4b63d</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>inopia</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100% 6-month tbills, lol :)</td>\n",
       "      <td>100% 6-month tbills, lol :)</td>\n",
       "      <td>0.85000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['%', 'lol']</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-11 21:38:43</td>\n",
       "      <td>mha0zp1</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>thenuttyhazlenut</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>|Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...</td>\n",
       "      <td>|Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...</td>\n",
       "      <td>0.49250</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Design|12.75', '|PBR|Petroleo', '|WISE', 'in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comment</td>\n",
       "      <td>2025-03-13 15:57:56</td>\n",
       "      <td>mhl2a2j</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rate My Portfolio - r/Stocks Quarterly Thread ...</td>\n",
       "      <td>elgrandorado</td>\n",
       "      <td>https://www.reddit.com/r/stocks/comments/1j0w7...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ticker &amp; %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...</td>\n",
       "      <td>Ticker &amp; %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['ASML', 'MA', 'NTDOY', 'SPGI', 'V', 'GOOG', '...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type             datetime  post_id subreddit  \\\n",
       "0     post  2025-03-01 10:00:42  1j0w73o    stocks   \n",
       "1  comment  2025-03-04 13:54:48  mfykqf0    stocks   \n",
       "2  comment  2025-03-11 00:18:18  mh4b63d    stocks   \n",
       "3  comment  2025-03-11 21:38:43  mha0zp1    stocks   \n",
       "4  comment  2025-03-13 15:57:56  mhl2a2j    stocks   \n",
       "\n",
       "                                               title            author  \\\n",
       "0  Rate My Portfolio - r/Stocks Quarterly Thread ...     AutoModerator   \n",
       "1  Rate My Portfolio - r/Stocks Quarterly Thread ...          Jimlad73   \n",
       "2  Rate My Portfolio - r/Stocks Quarterly Thread ...            inopia   \n",
       "3  Rate My Portfolio - r/Stocks Quarterly Thread ...  thenuttyhazlenut   \n",
       "4  Rate My Portfolio - r/Stocks Quarterly Thread ...      elgrandorado   \n",
       "\n",
       "                                                 url  upvotes  downvotes  \\\n",
       "0  https://www.reddit.com/r/stocks/comments/1j0w7...       79        4.0   \n",
       "1  https://www.reddit.com/r/stocks/comments/1j0w7...       12        NaN   \n",
       "2  https://www.reddit.com/r/stocks/comments/1j0w7...       11        NaN   \n",
       "3  https://www.reddit.com/r/stocks/comments/1j0w7...        6        NaN   \n",
       "4  https://www.reddit.com/r/stocks/comments/1j0w7...        7        NaN   \n",
       "\n",
       "   upvote_ratio                                               body  \\\n",
       "0          0.95  Please use this thread to discuss your portfol...   \n",
       "1           NaN                           100% S&P and bricking it   \n",
       "2           NaN                        100% 6-month tbills, lol :)   \n",
       "3           NaN  |Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...   \n",
       "4           NaN  Ticker & %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...   \n",
       "\n",
       "                                        cleaned_body  subjectivity  polarity  \\\n",
       "0  Please use this thread to discuss your portfol...       0.36803  0.176667   \n",
       "1                           100% S&P and bricking it       0.00000  0.000000   \n",
       "2                        100% 6-month tbills, lol :)       0.85000  0.650000   \n",
       "3  |Ticker|Company|Allocation|\\n|:-|:-|:-|\\n|ACGL...       0.49250  0.267500   \n",
       "4  Ticker & %\\n\\nSPGI: 18.2%\\n\\nASML: 15.2%\\n\\nGO...       0.00000  0.000000   \n",
       "\n",
       "  sentiment                                           entities  label  \n",
       "0       1.0  ['Losing', 'paper', 'portfolio', '%', '/', 'bo...    0.0  \n",
       "1   neutral                             ['S', 'P', '%', 'S&P']    0.0  \n",
       "2       1.0                                       ['%', 'lol']    0.0  \n",
       "3       1.0  ['Design|12.75', '|PBR|Petroleo', '|WISE', 'in...    1.0  \n",
       "4   neutral  ['ASML', 'MA', 'NTDOY', 'SPGI', 'V', 'GOOG', '...    0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combining the labelled data \n",
    "# Load the datasets\n",
    "df1 = pd.read_csv('/Users/jaredog/Downloads/git code/SC4021-Info-retrieval/ClassificationNew/JupyterNotebook/Sentimented(Neutral).csv')  # Replace with your dataset file paths\n",
    "df2 = pd.read_csv('/Users/jaredog/Downloads/git code/SC4021-Info-retrieval/ClassificationNew/JupyterNotebook/Labelled.csv')\n",
    "\n",
    "df1['sentiment'] = df1['sentiment'].replace({'positive': 1.0, 'negative': -1.0})\n",
    "# Merge the datasets based on 'post_id' column\n",
    "merged_df = pd.merge(df1, df2[['post_id', ' Label']], on='post_id', how='inner')\n",
    "\n",
    "# If you want to append the 'label' column from df2 to df1\n",
    "df1['label'] = merged_df[' Label']\n",
    "#df1 = df1[df1['label'] != 0.0]\n",
    "#df1 = df1.dropna(subset=['label']) #changed\n",
    "\n",
    "# Save the resulting dataframe to a new CSV file\n",
    "df1.to_csv('Sentimented+Labelled(neutral).csv', index=False)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ede4112e-bf50-4b09-a034-e559aeee7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df1\n",
    "\n",
    "# Step 1: Vectorize the cleaned text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# make sure no NaNs, and everything is a str\n",
    "data['cleaned_body'] = data['cleaned_body'].fillna(\"\").astype(str) #added\n",
    "\n",
    "X_text = vectorizer.fit_transform(data['cleaned_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69dde612-43c9-4da2-a55d-b2148740e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add subjectivity and polarity as features\n",
    "X_features = data[['subjectivity', 'polarity']].values\n",
    "\n",
    "# Combine the text features and sentiment features\n",
    "from scipy.sparse import hstack\n",
    "X_combined = hstack([X_text, X_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c29fd66-5da4-4ce7-ab15-4805d9a5fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert 'entities' column into binary features (one-hot encoding)\n",
    "mlb = MultiLabelBinarizer()\n",
    "entity_features = mlb.fit_transform(data['entities'].apply(eval))  # Convert string lists into actual lists\n",
    "\n",
    "# Combine entity features with the other features\n",
    "X_combined_with_entities = hstack([X_combined, entity_features])\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=50)\n",
    "X_combined_with_entities_pca = pca.fit_transform(X_combined_with_entities.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eff619f5-054b-424e-b5fa-59ca787c9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Apply KMeans clustering\n",
    "start_time = time.time()\n",
    "\n",
    "kmeans_with_entities = KMeans(n_clusters=5, random_state=42) #cluster 6 \n",
    "kmeans_with_entities.fit(X_combined_with_entities_pca)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "predicted_labels = kmeans_with_entities.labels_\n",
    "predicted_labels = predicted_labels.astype(int)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "records_classified_per_second = len(data) / total_time  # Assuming 'data' contains all records\n",
    "\n",
    "# Step 6: Map clusters to sentiment labels\n",
    "# Create a mapping based on the majority sentiment in each cluster\n",
    "cluster_sentiment_mapping = {}\n",
    "\n",
    "for cluster in range(kmeans_with_entities.n_clusters):\n",
    "    # Find the rows that belong to this cluster\n",
    "    cluster_rows = data[predicted_labels == cluster]\n",
    "    # Majority sentiment in the cluster\n",
    "    majority_sentiment = cluster_rows['label'].mode()[0]\n",
    "    # Map this cluster to the majority sentiment\n",
    "    cluster_sentiment_mapping[cluster] = majority_sentiment\n",
    "\n",
    "# Map predicted labels to sentiment labels\n",
    "mapped_sentiment_labels = [cluster_sentiment_mapping[label] for label in predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9ec65b6-441f-4f45-b584-0de4f128d204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9532\n",
      "Recall: 0.9508\n",
      "F1-Score: 0.9269\n",
      "Accuracy: 0.9508\n",
      "Records Classified per Second: 336870.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bp/974ksd917bx0z_pnb3yqq52h0000gn/T/ipykernel_57364/2961607586.py:3: RuntimeWarning: invalid value encountered in cast\n",
      "  true_labels = np.array(true_labels, dtype=float).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Get the true labels for the remaining rows\n",
    "true_labels = data['label']\n",
    "true_labels = np.array(true_labels, dtype=float).astype(int)\n",
    "\n",
    "# Step 8: Evaluate clustering performance using Precision, Recall, F1-Score\n",
    "precision = precision_score(true_labels, mapped_sentiment_labels, average='weighted', zero_division=1)  # Handling zero divisions\n",
    "recall = recall_score(true_labels, mapped_sentiment_labels, average='weighted', zero_division=1)\n",
    "f1 = f1_score(true_labels, mapped_sentiment_labels, average='weighted', zero_division=1)\n",
    "accuracy = accuracy_score(true_labels, mapped_sentiment_labels)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Records Classified per Second: {records_classified_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4f7ce31-4324-46f3-99c3-d6b6b5325e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-cluster centroid distances:\n",
      " [[0.         0.84018304 1.08723436 0.60411424 2.03532774]\n",
      " [0.84018304 0.         1.19097772 0.91319153 1.99876547]\n",
      " [1.08723436 1.19097772 0.         1.16667745 2.24825678]\n",
      " [0.60411424 0.91319153 1.16667745 0.         2.08940818]\n",
      " [2.03532774 1.99876547 2.24825678 2.08940818 0.        ]]\n",
      "Silhouette Score: 0.1204\n",
      "Davies-Bouldin Index: 2.6550\n",
      "Average Intra-cluster Similarity: 1.6433\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate Centroid Distances (Euclidean distance between centroids)\n",
    "centroids = kmeans_with_entities.cluster_centers_\n",
    "inter_cluster_distances = np.linalg.norm(centroids[:, np.newaxis] - centroids, axis=2)\n",
    "print(\"Inter-cluster centroid distances:\\n\", inter_cluster_distances)\n",
    "\n",
    "# 2. Calculate Silhouette Score\n",
    "silhouette_avg = silhouette_score(X_combined_with_entities_pca, predicted_labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "# 3. Calculate Davies-Bouldin Index\n",
    "davies_bouldin = davies_bouldin_score(X_combined_with_entities_pca, predicted_labels)\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "\n",
    "# 4. Calculate Average Intra-cluster Similarity (average distance within clusters)\n",
    "# Intra-cluster distance (average distance within each cluster)\n",
    "intra_cluster_similarity = []\n",
    "for cluster_id in range(kmeans_with_entities.n_clusters):\n",
    "    cluster_points = X_combined_with_entities_pca[predicted_labels == cluster_id]\n",
    "    cluster_center = centroids[cluster_id]\n",
    "    intra_cluster_similarity.append(np.mean(np.linalg.norm(cluster_points - cluster_center, axis=1)))\n",
    "\n",
    "average_intra_cluster_similarity = np.mean(intra_cluster_similarity)\n",
    "print(f\"Average Intra-cluster Similarity: {average_intra_cluster_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70b794db-da5a-4f98-a82e-897e1c594487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Accuracy Test Results:\n",
      "Accuracy: 0.3906\n",
      "Precision: 0.7497\n",
      "Recall: 0.3906\n",
      "F1-Score: 0.2865\n"
     ]
    }
   ],
   "source": [
    "#random test\n",
    "random_labels = np.random.choice([1, -1], size=len(data), replace=True)\n",
    "\n",
    "# Step 2: Get the predicted labels from your trained KMeans model\n",
    "predicted_labels = kmeans_with_entities.labels_\n",
    "\n",
    "# Step 3: Evaluate the clustering performance by comparing predicted labels to random labels\n",
    "accuracy = accuracy_score(random_labels, predicted_labels)\n",
    "precision = precision_score(random_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "recall = recall_score(random_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "f1 = f1_score(random_labels, predicted_labels, average='weighted', zero_division=1)\n",
    "\n",
    "# Step 4: Print the evaluation metrics\n",
    "print(f\"Random Accuracy Test Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de0447b4-850d-4372-a27c-4d14dfba8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torchfrom transformers \n",
    "import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e02f1609-126e-4ac8-aebb-3876c6b63838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings for a given text\n",
    "def get_bert_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        # Tokenize and encode the text\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        # Get the embeddings from BERT (the output of the last hidden layer)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Extract the last hidden state (embeddings)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        # Get the mean of all token embeddings to get the sentence embedding\n",
    "        sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze().numpy()\n",
    "        embeddings.append(sentence_embedding)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Example usage to get BERT embeddings for your dataset\n",
    "texts = data['cleaned_body'].tolist()  # Assuming you have a column 'cleaned_body'\n",
    "bert_embeddings = get_bert_embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "105071ca-47c4-4921-82c9-f54c088aa648",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)\n",
    "bert_embeddings_reduced = pca.fit_transform(bert_embeddings)\n",
    "\n",
    "# Step 5: Apply KMeans clustering using BERT embeddings\n",
    "kmeans_with_bert = KMeans(n_clusters=6, random_state=42)\n",
    "kmeans_with_bert.fit(bert_embeddings_reduced)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "predicted_labels_bert = kmeans_with_bert.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "daa88304-d2d8-4f90-bde3-072721cffe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████| 361/361 [00:00<00:00, 2023.24 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Fine-tune BERT for sentiment analysis\u001b[39;00m\n\u001b[32m      9\u001b[39m model_for_sentiment = BertForSequenceClassification.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m'\u001b[39m, num_labels=\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# 2 labels (positive/negative)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m trainer = Trainer(\n\u001b[32m     21\u001b[39m     model=model_for_sentiment,\n\u001b[32m     22\u001b[39m     args=training_args,\n\u001b[32m     23\u001b[39m     train_dataset=dataset,\n\u001b[32m     24\u001b[39m     eval_dataset=dataset,  \u001b[38;5;66;03m# Or use a separate validation dataset\u001b[39;00m\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:132\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, tp_size, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1761\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1759\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1760\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1761\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[32m   1764\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.average_tokens_across_devices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:2297\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2293\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2294\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2295\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2296\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/utils/generic.py:67\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, objtype)\u001b[39m\n\u001b[32m     65\u001b[39m cached = \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:2167\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2167\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2168\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2169\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2170\u001b[39m         )\n\u001b[32m   2171\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2172\u001b[39m accelerator_state_kwargs = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset for fine-tuning\n",
    "\n",
    "dataset = Dataset.from_pandas(data)  # Assuming your data is in a Pandas DataFrame\n",
    "dataset = dataset.map(lambda x: tokenizer(x['cleaned_body'], truncation=True, padding=True, max_length=512))\n",
    "\n",
    "# Fine-tune BERT for sentiment analysis\n",
    "model_for_sentiment = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 labels (positive/negative)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=200,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_for_sentiment,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,  # Or use a separate validation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2eb094aa-d3e3-41e3-80ac-940817c5e496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS backend\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01ad6f85-159f-43eb-b27d-0ed917ab8a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d100cbe-7960-4af3-b7e1-ed5ac81151f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4021",
   "language": "python",
   "name": "sc4021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
