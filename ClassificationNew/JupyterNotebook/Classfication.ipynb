{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f8da88-ef68-4226-a519-e2df983c344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24517bc3-4f2c-4131-b5cc-2e55632da6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4353fa0c-5283-4f87-a2d0-26491fd30363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f4120c-3c50-411d-83dc-8d3a3a0ae2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7377dbfc-1e20-414a-b3ff-f6f82cde0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# Function to clean text: Tokenization, Lemmatization, and Stopword removal\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the text is a string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        \n",
    "        # Remove URLs using a regular expression\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        \n",
    "        tokens = word_tokenize(text)  # Tokenize the text\n",
    "\n",
    "        # Lemmatize each token\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        # Additional preprocessing (like removing stopwords)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens_without_stopwords = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "\n",
    "        # Rejoin tokens into a single string before passing to TextBlob\n",
    "        cleaned_text = \" \".join(tokens_without_stopwords)\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        # If the text is not a string (e.g., NaN or float), return an empty string or a default value\n",
    "        return \"\"  # Or return a string like 'Invalid Text' if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7416ae-0f0d-4b45-bffb-f7f3c77f30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    # Step 1: Use spaCy to extract entities (like companies, products, etc.)\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]  # Extracting organizations\n",
    "\n",
    "    # Step 2: Apply POS tagging to the tokens in the text\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    pos_tags = pos_tag(tokens)  # Get POS tags for the tokens\n",
    "\n",
    "    # Step 3: Extract proper nouns (NNP) or important nouns (NN) using POS tagging\n",
    "    nouns = [word for word, tag in pos_tags if tag in [\"NNP\",\"NN\"]]  # Extracting proper nouns and common nouns\n",
    "\n",
    "    # Combine NER and POS results\n",
    "    refined_entities = list(set(entities + nouns))  # Combine and remove duplicates\n",
    "    return refined_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a55edd0-42f8-49c0-a87c-06da48902375",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Function to load data from Excel\n",
    "def load_data_from_excel(file_path):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Assuming the Excel file has columns 'text' for the comments\n",
    "    texts = df['comment_body'].tolist()  # List of texts (Reddit posts)\n",
    "    return df, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91bb5860-3a37-411a-a439-5e81e8417df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sentiment analysis with TextBlob\n",
    "def analyze_sentiment_with_textblob(texts):\n",
    "    subjectivity = []\n",
    "    polarity = []\n",
    "    for text in texts:\n",
    "        blob = TextBlob(text)\n",
    "        subjectivity.append(blob.sentiment.subjectivity)\n",
    "        polarity.append(blob.sentiment.polarity)\n",
    "    return subjectivity, polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd81035e-9688-4f7f-b838-0cfd54489cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Time: 6.1746 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load data from Excel sheet\n",
    "file_path = \"/Users/jaredog/Downloads/Scrapedv2.csv\"  # Replace with your actual Excel file path\n",
    "df, texts = load_data_from_excel(file_path)\n",
    "\n",
    "# --- 1. Preprocess the text ---\n",
    "start_time = time.time()\n",
    "cleaned_texts = [clean_text(text) for text in texts]\n",
    "df['cleaned_comment'] = cleaned_texts\n",
    "preprocessing_time = time.time() - start_time\n",
    "print(f\"Preprocessing Time: {preprocessing_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37830680-052e-41ad-a8db-77285202f7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Time: 2.4401 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Sentiment Analysis with TextBlob ---\n",
    "start_time = time.time()\n",
    "subjectivity, polarity = analyze_sentiment_with_textblob(cleaned_texts)\n",
    "sentiment_analysis_time = time.time() - start_time\n",
    "print(f\"Sentiment Analysis Time: {sentiment_analysis_time:.4f} seconds\")\n",
    "\n",
    "# Append the sentiment analysis results to the dataframe\n",
    "df['subjectivity'] = subjectivity\n",
    "df['polarity'] = polarity\n",
    "\n",
    "# Determine the overall sentiment (positive, negative, neutral)\n",
    "sentiment_labels = []\n",
    "for polarity_score in polarity:\n",
    "    if polarity_score > 0:\n",
    "        sentiment_labels.append('positive')\n",
    "    elif polarity_score < 0:\n",
    "        sentiment_labels.append('negative')\n",
    "    else:\n",
    "        sentiment_labels.append('neutral')\n",
    "\n",
    "df['sentiment'] = sentiment_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d99a3d-e310-43df-98a9-857afb21e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Remove neutral sentiment rows ---\n",
    "df = df[df['sentiment'] != 'neutral']  # Drop all rows where sentiment is 'neutral'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a27231a-4982-4124-b5a7-1860ed6d60ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Extraction Time: 229.3215 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_url</th>\n",
       "      <th>post_content</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>created</th>\n",
       "      <th>created_iso</th>\n",
       "      <th>readable_date</th>\n",
       "      <th>cleaned_comment</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tigerbrokers_official</td>\n",
       "      <td>TigerGPT Upgrades with DeepSeek-R1 and looks s...</td>\n",
       "      <td>1isb5q5</td>\n",
       "      <td>https://www.reddit.com/gallery/1isb5q5</td>\n",
       "      <td>[No text content]</td>\n",
       "      <td>mdz914f</td>\n",
       "      <td>yeah, i saw the launch news [from Reuters, Tig...</td>\n",
       "      <td>Passionjason</td>\n",
       "      <td>1</td>\n",
       "      <td>1740141702</td>\n",
       "      <td>2025-02-21 12:41:42</td>\n",
       "      <td>21 February 2025</td>\n",
       "      <td>yeah , saw launch news [ reuters , tiger broke...</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.223333</td>\n",
       "      <td>positive</td>\n",
       "      <td>[https, news, Tiger Brokers, Reuters, DeepSeek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>webull</td>\n",
       "      <td>Official referral thread</td>\n",
       "      <td>yvxdse</td>\n",
       "      <td>https://www.reddit.com/r/Webull/comments/yvxds...</td>\n",
       "      <td>Get your referral code or share one for someon...</td>\n",
       "      <td>lfv8ptv</td>\n",
       "      <td>Do you like money? That’s so weird so do I! We...</td>\n",
       "      <td>SirDouchebagTheThird</td>\n",
       "      <td>1</td>\n",
       "      <td>1722456593</td>\n",
       "      <td>2024-07-31 20:09:53</td>\n",
       "      <td>31 July 2024</td>\n",
       "      <td>like money ? ’ weird ! much common . kissed re...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-0.462500</td>\n",
       "      <td>negative</td>\n",
       "      <td>[//a.webull.com/NwcjDmTx8qsTj6ALhZ, https, 😚, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>webull</td>\n",
       "      <td>Official referral thread</td>\n",
       "      <td>yvxdse</td>\n",
       "      <td>https://www.reddit.com/r/Webull/comments/yvxds...</td>\n",
       "      <td>Get your referral code or share one for someon...</td>\n",
       "      <td>kuld53j</td>\n",
       "      <td>If anyone is still looking for a webull offer,...</td>\n",
       "      <td>Solid_Subject</td>\n",
       "      <td>1</td>\n",
       "      <td>1710283740</td>\n",
       "      <td>2024-03-12 22:49:00</td>\n",
       "      <td>12 March 2024</td>\n",
       "      <td>anyone still looking webull offer , one pretty...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>positive</td>\n",
       "      <td>[offer, https, anyone, //a.webull.com/TfjQ9yCQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>webull</td>\n",
       "      <td>Official referral thread</td>\n",
       "      <td>yvxdse</td>\n",
       "      <td>https://www.reddit.com/r/Webull/comments/yvxds...</td>\n",
       "      <td>Get your referral code or share one for someon...</td>\n",
       "      <td>kt15bq7</td>\n",
       "      <td>Not only can you get 75 free fractional shares...</td>\n",
       "      <td>taegha</td>\n",
       "      <td>1</td>\n",
       "      <td>1709402773</td>\n",
       "      <td>2024-03-02 18:06:13</td>\n",
       "      <td>02 March 2024</td>\n",
       "      <td>get 75 free fractional share , also best frien...</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>positive</td>\n",
       "      <td>[friend, https, //a.webull.com/TfjynhN7H8LJpyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>webull</td>\n",
       "      <td>Official referral thread</td>\n",
       "      <td>yvxdse</td>\n",
       "      <td>https://www.reddit.com/r/Webull/comments/yvxds...</td>\n",
       "      <td>Get your referral code or share one for someon...</td>\n",
       "      <td>mksi2wj</td>\n",
       "      <td>Unlock the stock market’s hidden treasure 👋👋👋 ...</td>\n",
       "      <td>nuddermado</td>\n",
       "      <td>1</td>\n",
       "      <td>1743473047</td>\n",
       "      <td>2025-04-01 2:04:07</td>\n",
       "      <td>01 April 2025</td>\n",
       "      <td>unlock stock market ’ hidden treasure 👋👋👋 sign...</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.136458</td>\n",
       "      <td>positive</td>\n",
       "      <td>[’, https, referral, house, market, Street, Si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               subreddit                                              title  \\\n",
       "0  tigerbrokers_official  TigerGPT Upgrades with DeepSeek-R1 and looks s...   \n",
       "1                 webull                           Official referral thread   \n",
       "2                 webull                           Official referral thread   \n",
       "3                 webull                           Official referral thread   \n",
       "4                 webull                           Official referral thread   \n",
       "\n",
       "   post_id                                           post_url  \\\n",
       "0  1isb5q5             https://www.reddit.com/gallery/1isb5q5   \n",
       "1   yvxdse  https://www.reddit.com/r/Webull/comments/yvxds...   \n",
       "2   yvxdse  https://www.reddit.com/r/Webull/comments/yvxds...   \n",
       "3   yvxdse  https://www.reddit.com/r/Webull/comments/yvxds...   \n",
       "4   yvxdse  https://www.reddit.com/r/Webull/comments/yvxds...   \n",
       "\n",
       "                                        post_content comment_id  \\\n",
       "0                                  [No text content]    mdz914f   \n",
       "1  Get your referral code or share one for someon...    lfv8ptv   \n",
       "2  Get your referral code or share one for someon...    kuld53j   \n",
       "3  Get your referral code or share one for someon...    kt15bq7   \n",
       "4  Get your referral code or share one for someon...    mksi2wj   \n",
       "\n",
       "                                        comment_body        comment_author  \\\n",
       "0  yeah, i saw the launch news [from Reuters, Tig...          Passionjason   \n",
       "1  Do you like money? That’s so weird so do I! We...  SirDouchebagTheThird   \n",
       "2  If anyone is still looking for a webull offer,...         Solid_Subject   \n",
       "3  Not only can you get 75 free fractional shares...                taegha   \n",
       "4  Unlock the stock market’s hidden treasure 👋👋👋 ...            nuddermado   \n",
       "\n",
       "   comment_score     created          created_iso     readable_date  \\\n",
       "0              1  1740141702  2025-02-21 12:41:42  21 February 2025   \n",
       "1              1  1722456593  2024-07-31 20:09:53      31 July 2024   \n",
       "2              1  1710283740  2024-03-12 22:49:00     12 March 2024   \n",
       "3              1  1709402773  2024-03-02 18:06:13     02 March 2024   \n",
       "4              1  1743473047   2025-04-01 2:04:07     01 April 2025   \n",
       "\n",
       "                                     cleaned_comment  subjectivity  polarity  \\\n",
       "0  yeah , saw launch news [ reuters , tiger broke...      0.453333  0.223333   \n",
       "1  like money ? ’ weird ! much common . kissed re...      0.750000 -0.462500   \n",
       "2  anyone still looking webull offer , one pretty...      0.800000  0.475000   \n",
       "3  get 75 free fractional share , also best frien...      0.475000  0.375000   \n",
       "4  unlock stock market ’ hidden treasure 👋👋👋 sign...      0.558333  0.136458   \n",
       "\n",
       "  sentiment                                           entities  \n",
       "0  positive  [https, news, Tiger Brokers, Reuters, DeepSeek...  \n",
       "1  negative  [//a.webull.com/NwcjDmTx8qsTj6ALhZ, https, 😚, ...  \n",
       "2  positive  [offer, https, anyone, //a.webull.com/TfjQ9yCQ...  \n",
       "3  positive  [friend, https, //a.webull.com/TfjynhN7H8LJpyt...  \n",
       "4  positive  [’, https, referral, house, market, Street, Si...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 4. Named Entity Recognition ---\n",
    "start_time = time.time()\n",
    "# Apply NER only to the remaining (non-neutral) rows\n",
    "# make sure no NaNs, and everything is a str\n",
    "\n",
    "remaining_texts = df['comment_body'].tolist()  # Only the rows with non-neutral sentiment\n",
    "entities = [extract_entities(text) for text in remaining_texts]\n",
    "entity_extraction_time = time.time() - start_time\n",
    "print(f\"Entity Extraction Time: {entity_extraction_time:.4f} seconds\")\n",
    "\n",
    "# Append extracted entities to the dataframe\n",
    "df['entities'] = entities\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67af5bd8-7d5b-4835-85fd-a2d57b588c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>cleaned_comment</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>webull</td>\n",
       "      <td>1jy0o2j</td>\n",
       "      <td>That’s one thing about Fidelity that stands ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>’ one thing fidelity stand , top notch custome...</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>positive</td>\n",
       "      <td>[customer, time, Fidelity, help, service, some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>webull</td>\n",
       "      <td>1jy0o2j</td>\n",
       "      <td>I have Fidelity and Webull and I prefer Fideli...</td>\n",
       "      <td>1</td>\n",
       "      <td>fidelity webull prefer fidelity . educational ...</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>positive</td>\n",
       "      <td>[customer, Fidelity, corp, planning, Webull, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>webull</td>\n",
       "      <td>1jy0o2j</td>\n",
       "      <td>It could use some twerking, but it's far from ...</td>\n",
       "      <td>3</td>\n",
       "      <td>could use twerking , 's far bad imo . although...</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>negative</td>\n",
       "      <td>[twerking, IMO, Fidelity, curve, app, learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>webull</td>\n",
       "      <td>1jy0o2j</td>\n",
       "      <td>Schwab was worse than both at one point</td>\n",
       "      <td>2</td>\n",
       "      <td>schwab wa worse one point</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>negative</td>\n",
       "      <td>[point, Schwab]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>webull</td>\n",
       "      <td>1jy0o2j</td>\n",
       "      <td>Facts. But like someone else said, their mobil...</td>\n",
       "      <td>1</td>\n",
       "      <td>fact . like someone else said , mobile ui blee...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>positive</td>\n",
       "      <td>[someone, Webull, UI, app]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>webull</td>\n",
       "      <td>1jy0o2j</td>\n",
       "      <td>Agree to disagree. Even with the huge improvem...</td>\n",
       "      <td>2</td>\n",
       "      <td>agree disagree . even huge improvement ’ made ...</td>\n",
       "      <td>0.494444</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>positive</td>\n",
       "      <td>[use, twerking, ease, Tasty, Agree, year, Webu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>webull</td>\n",
       "      <td>1jy0o2j</td>\n",
       "      <td>Definitely agree to disagree as we have differ...</td>\n",
       "      <td>2</td>\n",
       "      <td>definitely agree disagree different preference...</td>\n",
       "      <td>0.527500</td>\n",
       "      <td>0.193333</td>\n",
       "      <td>positive</td>\n",
       "      <td>[week, performance, multi-use, chain, RH, brok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>webull</td>\n",
       "      <td>1jxvayh</td>\n",
       "      <td>Got to love futures you never have this issue ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got love future never issue instant deposit time</td>\n",
       "      <td>0.463889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>positive</td>\n",
       "      <td>[time, deposit, issue, Got]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>webull</td>\n",
       "      <td>1jy2dyw</td>\n",
       "      <td>A lot of us have seen the same absurd posts ag...</td>\n",
       "      <td>12</td>\n",
       "      <td>lot u seen absurd post . daily assault webull ...</td>\n",
       "      <td>0.440146</td>\n",
       "      <td>-0.107738</td>\n",
       "      <td>negative</td>\n",
       "      <td>[customer, week, life, Please, support, minute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>webull</td>\n",
       "      <td>1jy2dyw</td>\n",
       "      <td>I specifically stated that I \"recently\" just s...</td>\n",
       "      <td>2</td>\n",
       "      <td>specifically stated `` recently '' started usi...</td>\n",
       "      <td>0.505303</td>\n",
       "      <td>-0.082955</td>\n",
       "      <td>negative</td>\n",
       "      <td>[output, time, lot, amount, hand, paragraph, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit  post_id                                       comment_body  \\\n",
       "106    webull  1jy0o2j  That’s one thing about Fidelity that stands ou...   \n",
       "107    webull  1jy0o2j  I have Fidelity and Webull and I prefer Fideli...   \n",
       "108    webull  1jy0o2j  It could use some twerking, but it's far from ...   \n",
       "109    webull  1jy0o2j            Schwab was worse than both at one point   \n",
       "110    webull  1jy0o2j  Facts. But like someone else said, their mobil...   \n",
       "111    webull  1jy0o2j  Agree to disagree. Even with the huge improvem...   \n",
       "112    webull  1jy0o2j  Definitely agree to disagree as we have differ...   \n",
       "113    webull  1jxvayh  Got to love futures you never have this issue ...   \n",
       "114    webull  1jy2dyw  A lot of us have seen the same absurd posts ag...   \n",
       "115    webull  1jy2dyw  I specifically stated that I \"recently\" just s...   \n",
       "\n",
       "     comment_score                                    cleaned_comment  \\\n",
       "106              5  ’ one thing fidelity stand , top notch custome...   \n",
       "107              1  fidelity webull prefer fidelity . educational ...   \n",
       "108              3  could use twerking , 's far bad imo . although...   \n",
       "109              2                          schwab wa worse one point   \n",
       "110              1  fact . like someone else said , mobile ui blee...   \n",
       "111              2  agree disagree . even huge improvement ’ made ...   \n",
       "112              2  definitely agree disagree different preference...   \n",
       "113              1   got love future never issue instant deposit time   \n",
       "114             12  lot u seen absurd post . daily assault webull ...   \n",
       "115              2  specifically stated `` recently '' started usi...   \n",
       "\n",
       "     subjectivity  polarity sentiment  \\\n",
       "106      0.404762  0.309524  positive   \n",
       "107      0.358333  0.250000  positive   \n",
       "108      0.622222 -0.133333  negative   \n",
       "109      0.600000 -0.400000  negative   \n",
       "110      0.600000  0.500000  positive   \n",
       "111      0.494444  0.200000  positive   \n",
       "112      0.527500  0.193333  positive   \n",
       "113      0.463889  0.166667  positive   \n",
       "114      0.440146 -0.107738  negative   \n",
       "115      0.505303 -0.082955  negative   \n",
       "\n",
       "                                              entities  \n",
       "106  [customer, time, Fidelity, help, service, some...  \n",
       "107  [customer, Fidelity, corp, planning, Webull, r...  \n",
       "108  [twerking, IMO, Fidelity, curve, app, learning...  \n",
       "109                                    [point, Schwab]  \n",
       "110                         [someone, Webull, UI, app]  \n",
       "111  [use, twerking, ease, Tasty, Agree, year, Webu...  \n",
       "112  [week, performance, multi-use, chain, RH, brok...  \n",
       "113                        [time, deposit, issue, Got]  \n",
       "114  [customer, week, life, Please, support, minute...  \n",
       "115  [output, time, lot, amount, hand, paragraph, d...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sc = df\n",
    "sc = sc.drop(['title','post_url','post_content','comment_id','comment_author','created','created_iso','readable_date', ], axis=1)\n",
    "sc.iloc[100:110]  # Shows rows 100 to 109\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2225c763-97ff-4fcb-96bb-c4c848023266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified data saved to Sentimentedv2.csv\n",
      "\n",
      "--- Performance Metrics ---\n",
      "Total Time for Preprocessing, Sentiment Analysis, and Entity Extraction: 237.9361 seconds\n",
      "Records Classified per Second: 43.70 records/second\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Save the modified data back to CSV ---\n",
    "output_file_path = \"Sentimentedv2.csv\"\n",
    "df.to_csv(output_file_path, index=False)\n",
    "print(f\"Modified data saved to {output_file_path}\")\n",
    "\n",
    "# --- Performance Metrics ---\n",
    "total_time = preprocessing_time + sentiment_analysis_time + entity_extraction_time\n",
    "records_per_second = len(df) / total_time  # records classified per second\n",
    "\n",
    "print(f\"\\n--- Performance Metrics ---\")\n",
    "print(f\"Total Time for Preprocessing, Sentiment Analysis, and Entity Extraction: {total_time:.4f} seconds\")\n",
    "print(f\"Records Classified per Second: {records_per_second:.2f} records/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85b0c011-064c-4304-8004-349032bec376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bp/974ksd917bx0z_pnb3yqq52h0000gn/T/ipykernel_83505/3559181657.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df1['sentiment'] = df1['sentiment'].replace({'positive': 1.0, 'negative': -1.0})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_url</th>\n",
       "      <th>post_content</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>created</th>\n",
       "      <th>created_iso</th>\n",
       "      <th>readable_date</th>\n",
       "      <th>cleaned_comment</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>entities</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>webull</td>\n",
       "      <td>Tax lots on Webull</td>\n",
       "      <td>1ipge6z</td>\n",
       "      <td>https://www.reddit.com/r/Webull/comments/1ipge...</td>\n",
       "      <td>Since Webull and Robinhood use Apex clearing f...</td>\n",
       "      <td>md26tdl</td>\n",
       "      <td>Idk the answer to this, but I hope as well the...</td>\n",
       "      <td>DragonfruitLopsided</td>\n",
       "      <td>2</td>\n",
       "      <td>1739707315</td>\n",
       "      <td>2025-02-16 12:01:55</td>\n",
       "      <td>16 February 2025</td>\n",
       "      <td>idk answer , hope well . tax lot standard brok...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>['cost', 'lot', 'answer', 'Tax', 'Idk', 'broke...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>webull</td>\n",
       "      <td>Has anyone had this woe up this morning with 5...</td>\n",
       "      <td>1iohei6</td>\n",
       "      <td>https://i.redd.it/qfdcftkwawie1.jpeg</td>\n",
       "      <td>[No text content]</td>\n",
       "      <td>mcjcvrm</td>\n",
       "      <td>I posted about a similar issue yesterday eveni...</td>\n",
       "      <td>Difficult_Poetry_259</td>\n",
       "      <td>2</td>\n",
       "      <td>1739448730</td>\n",
       "      <td>2025-02-13 12:12:10</td>\n",
       "      <td>13 February 2025</td>\n",
       "      <td>posted similar issue yesterday evening . accou...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>['customer', 'switch', 'yesterday', 'solution'...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>webull</td>\n",
       "      <td>Has anyone had this woe up this morning with 5...</td>\n",
       "      <td>1iohei6</td>\n",
       "      <td>https://i.redd.it/qfdcftkwawie1.jpeg</td>\n",
       "      <td>[No text content]</td>\n",
       "      <td>mcjgop2</td>\n",
       "      <td>So you have two limits. There is a Day Trade l...</td>\n",
       "      <td>GunsouBono</td>\n",
       "      <td>9</td>\n",
       "      <td>1739450432</td>\n",
       "      <td>2025-02-13 12:40:32</td>\n",
       "      <td>13 February 2025</td>\n",
       "      <td>two limit . day trade limit ( 5k ) must close ...</td>\n",
       "      <td>0.490357</td>\n",
       "      <td>0.181071</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['line', 'day', 'limit', 'MSFT', 'fall', 'Day'...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>webull</td>\n",
       "      <td>Is this a bug?</td>\n",
       "      <td>1iohjsh</td>\n",
       "      <td>https://www.reddit.com/gallery/1iohjsh</td>\n",
       "      <td>[No text content]</td>\n",
       "      <td>mcjebpz</td>\n",
       "      <td>This makes three of us who have complained abo...</td>\n",
       "      <td>Difficult_Poetry_259</td>\n",
       "      <td>0</td>\n",
       "      <td>1739449393</td>\n",
       "      <td>2025-02-13 12:23:13</td>\n",
       "      <td>13 February 2025</td>\n",
       "      <td>make three u complained issue webull . please ...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>['customer', 'r/Webull', 'Please', '’', 'Trade...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>webull</td>\n",
       "      <td>I'm puzzled.</td>\n",
       "      <td>1io8c7g</td>\n",
       "      <td>https://i.redd.it/6devokbpdtie1.png</td>\n",
       "      <td>I'm trying to add my debit card and this pops ...</td>\n",
       "      <td>mchjvct</td>\n",
       "      <td>Exactly the same thing happen for me as well. ...</td>\n",
       "      <td>Siqk-</td>\n",
       "      <td>2</td>\n",
       "      <td>1739415535</td>\n",
       "      <td>2025-02-13 2:58:55</td>\n",
       "      <td>13 February 2025</td>\n",
       "      <td>exactly thing happen well . reached first time...</td>\n",
       "      <td>0.277976</td>\n",
       "      <td>0.094296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['customer', 'support', 'us ach/', 'happen', '...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit                                              title  post_id  \\\n",
       "340    webull                                 Tax lots on Webull  1ipge6z   \n",
       "341    webull  Has anyone had this woe up this morning with 5...  1iohei6   \n",
       "342    webull  Has anyone had this woe up this morning with 5...  1iohei6   \n",
       "343    webull                                     Is this a bug?  1iohjsh   \n",
       "344    webull                                       I'm puzzled.  1io8c7g   \n",
       "\n",
       "                                              post_url  \\\n",
       "340  https://www.reddit.com/r/Webull/comments/1ipge...   \n",
       "341               https://i.redd.it/qfdcftkwawie1.jpeg   \n",
       "342               https://i.redd.it/qfdcftkwawie1.jpeg   \n",
       "343             https://www.reddit.com/gallery/1iohjsh   \n",
       "344                https://i.redd.it/6devokbpdtie1.png   \n",
       "\n",
       "                                          post_content comment_id  \\\n",
       "340  Since Webull and Robinhood use Apex clearing f...    md26tdl   \n",
       "341                                  [No text content]    mcjcvrm   \n",
       "342                                  [No text content]    mcjgop2   \n",
       "343                                  [No text content]    mcjebpz   \n",
       "344  I'm trying to add my debit card and this pops ...    mchjvct   \n",
       "\n",
       "                                          comment_body        comment_author  \\\n",
       "340  Idk the answer to this, but I hope as well the...   DragonfruitLopsided   \n",
       "341  I posted about a similar issue yesterday eveni...  Difficult_Poetry_259   \n",
       "342  So you have two limits. There is a Day Trade l...            GunsouBono   \n",
       "343  This makes three of us who have complained abo...  Difficult_Poetry_259   \n",
       "344  Exactly the same thing happen for me as well. ...                 Siqk-   \n",
       "\n",
       "     comment_score     created          created_iso     readable_date  \\\n",
       "340              2  1739707315  2025-02-16 12:01:55  16 February 2025   \n",
       "341              2  1739448730  2025-02-13 12:12:10  13 February 2025   \n",
       "342              9  1739450432  2025-02-13 12:40:32  13 February 2025   \n",
       "343              0  1739449393  2025-02-13 12:23:13  13 February 2025   \n",
       "344              2  1739415535   2025-02-13 2:58:55  13 February 2025   \n",
       "\n",
       "                                       cleaned_comment  subjectivity  \\\n",
       "340  idk answer , hope well . tax lot standard brok...      0.450000   \n",
       "341  posted similar issue yesterday evening . accou...      0.400000   \n",
       "342  two limit . day trade limit ( 5k ) must close ...      0.490357   \n",
       "343  make three u complained issue webull . please ...      0.200000   \n",
       "344  exactly thing happen well . reached first time...      0.277976   \n",
       "\n",
       "     polarity  sentiment                                           entities  \\\n",
       "340 -0.250000       -1.0  ['cost', 'lot', 'answer', 'Tax', 'Idk', 'broke...   \n",
       "341 -0.050000       -1.0  ['customer', 'switch', 'yesterday', 'solution'...   \n",
       "342  0.181071        1.0  ['line', 'day', 'limit', 'MSFT', 'fall', 'Day'...   \n",
       "343 -0.300000       -1.0  ['customer', 'r/Webull', 'Please', '’', 'Trade...   \n",
       "344  0.094296        1.0  ['customer', 'support', 'us ach/', 'happen', '...   \n",
       "\n",
       "     label  \n",
       "340   -1.0  \n",
       "341   -1.0  \n",
       "342   -1.0  \n",
       "343   -1.0  \n",
       "344   -1.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combining the labelled data \n",
    "# Load the datasets\n",
    "df1 = pd.read_csv('/Users/jaredog/Downloads/git code/SC4021-Info-retrieval/ClassificationNew/JupyterNotebook/Sentimentedv2.csv')  # Replace with your dataset file paths\n",
    "df2 = pd.read_csv('/Users/jaredog/Downloads/git code/SC4021-Info-retrieval/ClassificationNew/JupyterNotebook/Labelledv2.csv')\n",
    "\n",
    "df1['sentiment'] = df1['sentiment'].replace({'positive': 1.0, 'negative': -1.0})\n",
    "# Merge the datasets based on 'post_id' column\n",
    "merged_df = pd.merge(df1, df2[['post_id', 'label']], on='post_id', how='inner')\n",
    "\n",
    "# If you want to append the 'label' column from df2 to df1\n",
    "df1['label'] = merged_df['label']\n",
    "df1['label'] = pd.to_numeric(df1['label'], errors='coerce')  # Converts to float, turns non-numeric to NaN\n",
    "df1 = df1.dropna(subset=['label']) #changed\n",
    "df1 = df1[df1['label'] != 0.0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the resulting dataframe to a new CSV file\n",
    "df1.to_csv('Sentimented+Labelledv2.csv', index=False)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ede4112e-bf50-4b09-a034-e559aeee7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df1\n",
    "\n",
    "# Step 1: Vectorize the cleaned text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# make sure no NaNs, and everything is a str\n",
    "\n",
    "X_text = vectorizer.fit_transform(data['comment_body'])\n",
    "X_subreddit = vectorizer.fit_transform(data['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69dde612-43c9-4da2-a55d-b2148740e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Add subjectivity and polarity as features\n",
    "X_features = data[['subjectivity', 'polarity','comment_score']].values\n",
    "\n",
    "# Combine the text features and sentiment features\n",
    "from scipy.sparse import hstack\n",
    "X_combined = hstack([X_subreddit, X_text, X_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c29fd66-5da4-4ce7-ab15-4805d9a5fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert 'entities' column into binary features (one-hot encoding)\n",
    "mlb = MultiLabelBinarizer()\n",
    "entity_features = mlb.fit_transform(data['entities'].apply(eval))  # Convert string lists into actual lists\n",
    "\n",
    "# Combine entity features with the other features\n",
    "X_combined_with_entities = hstack([X_combined, entity_features])\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=50)\n",
    "X_combined_with_entities_pca = pca.fit_transform(X_combined_with_entities.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5250064b-8588-43ee-be2d-a20c72c3c88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6661016810941848\n",
      "Recall: 0.6499442586399108\n",
      "F1 Score: 0.6533954379136725\n",
      "Accuracy: 0.6499442586399108\n"
     ]
    }
   ],
   "source": [
    "#Testing with logistic regression\n",
    "# Filter only manually labeled rows\n",
    "labeled_data = data[~data['label'].isna()]\n",
    "X_labeled = X_combined_with_entities_pca[~data['label'].isna()]\n",
    "y_labeled = labeled_data['label'].astype(int)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a supervised classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted', zero_division=1))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted', zero_division=1))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted', zero_division=1))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eff619f5-054b-424e-b5fa-59ca787c9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Apply KMeans clustering\n",
    "start_time = time.time()\n",
    "\n",
    "kmeans_with_entities = KMeans(n_clusters=3, random_state=42) #best 3\n",
    "kmeans_with_entities.fit(X_combined_with_entities_pca)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "predicted_labels = kmeans_with_entities.labels_\n",
    "predicted_labels = predicted_labels.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "records_classified_per_second = len(data) / total_time  # Assuming 'data' contains all records\n",
    "\n",
    "# Step 6: Map clusters to sentiment labels\n",
    "# Create a mapping based on the majority sentiment in each cluster\n",
    "cluster_sentiment_mapping = {}\n",
    "\n",
    "for cluster in range(kmeans_with_entities.n_clusters):\n",
    "    # Find the rows that belong to this cluster\n",
    "    cluster_rows = data[predicted_labels == cluster]\n",
    "    # Majority sentiment in the cluster\n",
    "    majority_sentiment = cluster_rows['label'].mode()[0]\n",
    "    # Map this cluster to the majority sentiment\n",
    "    cluster_sentiment_mapping[cluster] = majority_sentiment\n",
    "\n",
    "# Map predicted labels to sentiment labels\n",
    "mapped_sentiment_labels = [cluster_sentiment_mapping[label] for label in predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9ec65b6-441f-4f45-b584-0de4f128d204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7603\n",
      "Recall: 0.6016\n",
      "F1-Score: 0.4520\n",
      "Accuracy: 0.6016\n",
      "Records Classified per Second: 837142.82\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Get the true labels for the remaining rows\n",
    "true_labels = data['label']\n",
    "true_labels = np.array(true_labels, dtype=float).astype(int)\n",
    "\n",
    "# Step 8: Evaluate clustering performance using Precision, Recall, F1-Score\n",
    "precision = precision_score(true_labels, mapped_sentiment_labels, average='weighted', zero_division=1)  # Handling zero divisions\n",
    "recall = recall_score(true_labels, mapped_sentiment_labels, average='weighted', zero_division=1)\n",
    "f1 = f1_score(true_labels, mapped_sentiment_labels, average='weighted', zero_division=1)\n",
    "accuracy = accuracy_score(true_labels, mapped_sentiment_labels)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Records Classified per Second: {records_classified_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4f7ce31-4324-46f3-99c3-d6b6b5325e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-cluster centroid distances:\n",
      " [[ 0.         35.60791418  7.47917706]\n",
      " [35.60791418  0.         28.13704553]\n",
      " [ 7.47917706 28.13704553  0.        ]]\n",
      "Silhouette Score: 0.5512\n",
      "Davies-Bouldin Index: 0.6609\n",
      "Average Intra-cluster Similarity: 4.9168\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate Centroid Distances (Euclidean distance between centroids)\n",
    "centroids = kmeans_with_entities.cluster_centers_\n",
    "inter_cluster_distances = np.linalg.norm(centroids[:, np.newaxis] - centroids, axis=2)\n",
    "print(\"Inter-cluster centroid distances:\\n\", inter_cluster_distances)\n",
    "\n",
    "# 2. Calculate Silhouette Score\n",
    "silhouette_avg = silhouette_score(X_combined_with_entities_pca, predicted_labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "# 3. Calculate Davies-Bouldin Index\n",
    "davies_bouldin = davies_bouldin_score(X_combined_with_entities_pca, predicted_labels)\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "\n",
    "# 4. Calculate Average Intra-cluster Similarity (average distance within clusters)\n",
    "# Intra-cluster distance (average distance within each cluster)\n",
    "intra_cluster_similarity = []\n",
    "for cluster_id in range(kmeans_with_entities.n_clusters):\n",
    "    cluster_points = X_combined_with_entities_pca[predicted_labels == cluster_id]\n",
    "    cluster_center = centroids[cluster_id]\n",
    "    intra_cluster_similarity.append(np.mean(np.linalg.norm(cluster_points - cluster_center, axis=1)))\n",
    "\n",
    "average_intra_cluster_similarity = np.mean(intra_cluster_similarity)\n",
    "print(f\"Average Intra-cluster Similarity: {average_intra_cluster_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "70b794db-da5a-4f98-a82e-897e1c594487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Accuracy Test Results:\n",
      "Accuracy: 0.5097\n",
      "Precision: 0.7501\n",
      "Recall: 0.5097\n",
      "F1-Score: 0.3442\n"
     ]
    }
   ],
   "source": [
    "#random test\n",
    "random_labels = np.random.choice([1, -1], size=len(data), replace=True)\n",
    "\n",
    "# Step 2: Get the predicted labels from your trained KMeans model\n",
    "predicted_labels = kmeans_with_entities.labels_\n",
    "\n",
    "# Step 3: Evaluate the clustering performance by comparing predicted labels to random labels\n",
    "accuracy = accuracy_score(random_labels, mapped_sentiment_labels)\n",
    "precision = precision_score(random_labels, mapped_sentiment_labels, average='weighted', zero_division=1)\n",
    "recall = recall_score(random_labels, mapped_sentiment_labels, average='weighted', zero_division=1)\n",
    "f1 = f1_score(random_labels, mapped_sentiment_labels, average='weighted', zero_division=1)\n",
    "\n",
    "# Step 4: Print the evaluation metrics\n",
    "print(f\"Random Accuracy Test Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6288b-6280-4d5e-9fce-c6621d021e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab2ed0-083d-47c2-a0f3-152e9d4f012d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4021",
   "language": "python",
   "name": "sc4021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
